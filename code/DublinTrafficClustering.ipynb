{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dublin Traffic Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic sensors in Dublin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dublin employs the Sydney Coordinated Adaptive Traffic System (SCATS) to monitor traffic at key intersections and adapt traffic signals for high priority vehicles like public busses. Periodically, the city releases samples of the traffic counts that the SCATS system records on the SmartDublin open data store. This sample is from January to April 2012, so a lot has changed in Dublin regarding the transit system, traffic levels, and how traffic is collected. The data contains a table with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- *streetSegId* - street segment ids which can be matched to an accompanying spatial .gps file\n",
    "- *upperTime* - the timestamp losing the previous 6-minute aggregation\n",
    "- *armNumber* - an id for the arm where the sensor is located within a street segment id (not used)\n",
    "- *aggregateCount* - the number of vehicles detected by the sensor for the given time period (not used)\n",
    "- *flow* - ratio of the volume count to the maximum value in a 1-week sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, I will use the forthcoming sample from Jan-Apr 2020 and compare. For now, we can use this older sample to demonstrate the process of clustering time series and learn about the spatial and temporal patterns of commuting in Dublin.\n",
    "\n",
    "We are going to use the time series created by 4 months of 6-minute resolution data to find prototypical daily time series of 4 unique commuting patterns. Those patterns also exhibit spatial patterns, from which we can differentiate between regions of Dublin and how they experience daily traffic flows. We will do some R data table manipulation, Dynamic Time Warping to determine the 'distance' between sensors' time series, partitional clustering, and then finally ggplot and tmap to view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the data. Save the zipped folder in the same folder as your Jupyter notebook. Then extract into a new folder with the default name. The data is available from Smart Dublin [here](https://data.smartdublin.ie/dataset/volume-data-for-dublin-city-from-dublin-city-council-traffic-departments-scats-system/resource/b111de96-47ff-44fa-85d4-81155b8f2f83)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, we'll extract the .dis files (Oracle Discover workbook files, but R can read them as tables) and check for symmetry symmentry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/latex": [],
      "text/markdown": [],
      "text/plain": [
       "character(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filenames <- list.files(path=\"YOUR LOCAL FOLDER PATH HERE\",\n",
    "filenames <- list.files(path=\"/\",\n",
    "    pattern=\"*.dis\")\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allfiles <- list()\n",
    "\n",
    "for(file in filenames){\n",
    "allfiles[[file]]<-as.data.table(read.table(paste(\"YOUR LOCAL FOLDER PATH  AGAIN\",\n",
    "                                file,sep=\"\"),\n",
    "                                sep=\",\",\n",
    "                                header=TRUE,\n",
    "                                strip.white=TRUE))\n",
    "}\n",
    "\n",
    "# manually check for daily differences \n",
    "numobservations <- list()\n",
    "uniquesensors<-list()\n",
    "uniquetimestamps<-list()\n",
    "for(readfile in names(allfiles)){\n",
    "    numobservations[[readfile]] <- nrow(allfiles[[readfile]])\n",
    "    uniquesensors[[readfile]]<-length(unique(allfiles[[readfile]]$streetSegId))\n",
    "    uniquetimestamps[[readfile]]<-length(unique(allfiles[[readfile]]$upperTime))         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some discrepancies in the unique lists. The number of sensors changes, with several being added to the network throughout the 4 month time period. On one date, a couple of time are missing from the time series. Thus, the number of observations fluxuates. But when we create transposed versions of these table which are readable as time series objects for the tsclust package, these discrepancies will be accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create new tables for each day, with the timestamps in the columns and the street segments as the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in transposedfiles[[1]]: subscript out of bounds\n",
     "output_type": "error",
     "traceback": [
      "Error in transposedfiles[[1]]: subscript out of bounds\nTraceback:\n",
      "1. as.POSIXct(as.numeric(as.character(names(transposedfiles[[1]]))), \n .     origin = \"1970-01-01\", tz = \"GMT\")"
     ]
    }
   ],
   "source": [
    "#unique list of all of the identifiers of each sensor\n",
    "allSegIds<-list()\n",
    "#putting the sensor readings in a time series readable format\n",
    "transposedfiles<-list()\n",
    "for(fullfile in names(allfiles)) {\n",
    "    transpose <- dcast.data.table(allfiles[[fullfile]], \n",
    "                                  streetSegId ~ upperTime, fun.aggregate=mean, value.var=\"flow\")\n",
    "    streetSegIDs <- as.list(transpose[,1][[1]])\n",
    "    transpose <- transpose[,-1]\n",
    "    rownames(transpose) <- streetSegIDs\n",
    "    allSegIds<-c(allSegIds, streetSegIDs)\n",
    "    transposedfiles[[fullfile]]<-transpose\n",
    "}\n",
    "                        \n",
    "allSegIds<-unique(allSegIds)\n",
    "\n",
    "#convert text to dates and times and store in usable formats\n",
    "dates<-as.POSIXct(as.numeric(as.character(names(transposedfiles[[1]]))),\n",
    "                  origin=\"1970-01-01\",tz=\"GMT\")\n",
    "times <- lapply(dates, FUN=function(x) as.POSIXct(x, format=\"%H:%M:%S\"))\n",
    "timestamps<-colnames(transposedfiles[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list()\n"
     ]
    }
   ],
   "source": [
    "print(numobservations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
